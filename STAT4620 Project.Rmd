---
title: "STAT4620 Project"
author: "Catherine Ling.273"
date: "2025-12-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, echo = TRUE, message = FALSE)
```

```{r}
library(tidyverse)
library(GGally)
library(corrplot)
library(glmnet)
# install.packages("pls") # Run if not already installed, otherwise leave this commented out
library(pls)

ames <- read_csv("train.csv", show_col_types = FALSE)

glimpse(ames)
```

# Part I: Exploratory Data Analysis

## Dataset Introduction

The Ames Housing dataset contains residential home sales in Ames, Iowa from 2006–2010 and includes 79 predictors describing lot characteristics, house quality, building type, square footage, basement and garage features, and neighborhood information. The response variable for modeling is SalePrice, the sale price of each home in USD.

The dataset contains a mixture of numeric, ordinal, and nominal variables. Its initial state: 

```{r}
# summary statistics
summary(ames)

# split by variable type
numeric_vars <- ames %>% select(where(is.numeric))
categorical_vars <- ames %>% select(where(~ !is.numeric(.)))

```
## Missing Data

Several variables contain missing values, but many of these are structural (“feature not present”) rather than incomplete. For example, if a house has no basement, basement quality fields are NA.


```{r}
missing_summary <- ames %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "missing") %>%
  arrange(desc(missing))

missing_summary
```

Findings:

- Variables with very high missingness include PoolQC, MiscFeature, Alley, and Fence, which is expected because most homes do not have pools, miscellaneous features, alleys, and fences

- Basement and garage quality variables also include NAs for homes without basements or garages

- LotFrontage has ~18% missingness, representing true unobserved data rather than a structural “None”

```{r}
# convert known structural NAs to "None"
ames <- ames %>%
  mutate(
    Alley        = replace_na(Alley, "None"),
    PoolQC       = replace_na(PoolQC, "None"),
    Fence        = replace_na(Fence, "None"),
    MiscFeature  = replace_na(MiscFeature, "None"),
    FireplaceQu  = replace_na(FireplaceQu, "None"),
    GarageType   = replace_na(GarageType, "None"),
    GarageFinish = replace_na(GarageFinish, "None"),
    GarageQual   = replace_na(GarageQual, "None"),
    GarageCond   = replace_na(GarageCond, "None"),
    BsmtQual     = replace_na(BsmtQual, "None"),
    BsmtCond     = replace_na(BsmtCond, "None"),
    BsmtExposure = replace_na(BsmtExposure, "None"),
    BsmtFinType1 = replace_na(BsmtFinType1, "None"),
    BsmtFinType2 = replace_na(BsmtFinType2, "None")
  )
```

## Distribution of Variables 

**SalePrice**

One notable issue is that SalePrice is heavily right-skewed, which violates the normality assumption of linear regression. 

```{r}
# histograms
ggplot(ames, aes(x = SalePrice)) +
  geom_histogram(bins = 40, fill = "blue") +
  labs(title = "SalePrice")
```

A log-transformation produces a much more symmetric distribution:

```{r}
# log transformation to normalize
ames <- ames %>% mutate(LogSalePrice = log(SalePrice))

ggplot(ames, aes(x = LogSalePrice)) +
  geom_histogram(bins = 40, fill = "red") +
  labs(title = "log(SalePrice)")
```

Skewness suggests that data transformations (e.g., log) or models that don't require linearity would be best suited for this dataset. 

**Numeric Variables**

```{r}
key_numeric <- c("GrLivArea", "TotalBsmtSF", "GarageArea", "LotArea",
                 "X1stFlrSF", "X2ndFlrSF", "OverallQual")

key_numeric <- intersect(key_numeric, names(ames))

for (var in key_numeric) {
  print(
    ggplot(ames, aes_string(x = var)) +
      geom_histogram(bins = 40, fill = "gray") +
      labs(title = paste("Distribution of", var))
  )
}
```

## Relationships with SalePrice

**Size/area variables**
```{r}
# scatterplots for numeric predictors
size_vars <- intersect(c("GrLivArea", "TotalBsmtSF", "GarageArea", "X1stFlrSF"), names(ames))

for (var in size_vars) {
  print(
    ggplot(ames, aes_string(x = var, y = "SalePrice")) +
      geom_point(alpha = 0.5) +
      geom_smooth(method = "lm", se = FALSE) +
      labs(title = paste("SalePrice vs", var))
  )
}
```

- GrLivArea and TotalBsmtSF show strong positive linear trends.

- Outliers exist for very large homes (>4000 sq ft).


**Quality**
```{r}
# overallQual boxplot
if ("OverallQual" %in% names(ames)) {
  ggplot(ames, aes(x = factor(OverallQual), y = SalePrice)) +
    geom_boxplot() +
    labs(title = "SalePrice by OverallQual")
}
```

Higher OverallQual correlates strongly with higher prices, showing a step-like increase.


**Neighborhood**
```{r}
# neighborhood boxplot
if ("Neighborhood" %in% names(ames)) {
  ggplot(ames, aes(x = reorder(Neighborhood, SalePrice, FUN = median),
                   y = SalePrice)) +
    geom_boxplot() +
    coord_flip() +
    labs(title = "SalePrice by Neighborhood")
}
```

Neighborhoods differ substantially in median home value, implying they are relatively important predictors.

## Correlation & Collinearity

```{r}
cor_matrix <- numeric_vars %>% cor(use = "pairwise.complete.obs")

# correlation with SalePrice
cor_with_price <- sort(cor_matrix[,"SalePrice"], decreasing = TRUE)
cor_with_price

# top correlated predictors
top_corr <- names(head(cor_with_price, 10))
```

Strongest correlations: 
- OverallQual (0.79)
- GrLivArea (0.71)
- GarageCars, GarageArea, TotalBsmtSF, 1stFlrSF (~0.60 - 0.64)


Multicollinearity among top predictors: 

```{r}
cor_top <- numeric_vars %>%
  select(all_of(top_corr)) %>%
  cor(use = "pairwise.complete.obs")

# heatmap
corrplot::corrplot(cor_top, method = "color", type = "upper",
         tl.col = "black", tl.srt = 45)

# pairs plot
GGally::ggpairs(ames, columns = top_corr)

```

- GarageCars and GarageArea are highly collinear

- 1stFlrSF and TotalBsmtSF are strongly correlated

- Quality ratings like OverallQual correlate with other quality-related scores

Collinearity suggests models with regularization (Ridge/LASSO) or selective variable inclusion would work best. 

## Other Problems

**Zero-inflation** 

Several variables contain mostly zeros: 

```{r}
zero_vars <- intersect(c("PoolArea", "MiscVal", "ScreenPorch",
                         "EnclosedPorch", "LowQualFinSF"), names(ames))

for (var in zero_vars) {
  zero_prop <- mean(ames[[var]] == 0)
  cat(var, "- proportion zeros:", round(zero_prop, 3), "\n")
  
  print(
    ggplot(ames, aes_string(x = var)) +
      geom_histogram(bins = 40, fill = "gray50") +
      labs(title = paste("Distribution of", var))
  )
}
```

**Outliers**

```{r}
if ("GrLivArea" %in% names(ames)) {
  ggplot(ames, aes(x = GrLivArea, y = SalePrice)) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", se = FALSE) +
    labs(title = "Outlier Check: GrLivArea vs SalePrice")

  # view suspected outliers
  ames %>% filter(GrLivArea > 4000) %>%
    select(GrLivArea, SalePrice, OverallQual, Neighborhood)
}
```
Houses with very large living area (>4000 sq ft) appear to be outliers and may disproportionately influence linear regression


# Part II: Model Analysis - LASSO Regression


## Motivation for Using LASSO

The EDA revealed:

- Many predictors are highly correlated (e.g., GarageCars with GarageArea, 1stFlrSF with TotalBsmtSF).

- The dataset contains many potential predictors (~79).

- Some predictors may be irrelevant or redundant.

- Linear relationships appear strong, especially after transforming SalePrice.


Because of this, ordinary least squares (OLS) would be unstable and prone to overfitting due to multicollinearity and high dimensionality.

LASSO regression (Least Absolute Shrinkage and Selection Operator) performs:

- Regularization -> improves generalization

- Variable selection -> shrinks coefficients of unimportant variables to zero

- Handles multicollinearity better than OLS

Thus, LASSO is an appropriate modeling strategy for this dataset.


**The LASSO Model**

Given predictors $X_1, X_2, ..., X_p$ and response $y$, LASSO solves: 

$$
\hat{\beta} = \arg\min_\beta \left( \sum^n_{i=1}(y_i-\beta_0-\sum^p_{j=1} X_{ij} \beta_j)^2 + \lambda \sum^p_{j=1} |\beta_j| \right)
$$

where $\lambda \ge 0$ controls shrinkage.


## Data Preparation

Training data log-transform: 

```{r}
ames <- read_csv("train.csv") %>%
  mutate(
    log_price = log(SalePrice)
  )
```


Handling missing values: 

- for character predictors: replace NA with "None", then convert to factor
- for numeric predictors (other than Id, SalePrice, log_price): median impute

```{r}

ames_clean <- ames %>%
  # character -> "None" where NA
  mutate(across(where(is.character), ~ replace_na(., "None"))) %>%
  # now convert all characters to factors
  mutate(across(where(is.character), factor))

# numeric predictors (exclude Id, SalePrice, log_price)
num_pred <- names(ames_clean)[sapply(ames_clean, is.numeric) &
                                !(names(ames_clean) %in% c("Id", "SalePrice", "log_price"))]

ames_clean <- ames_clean %>%
  mutate(
    across(all_of(num_pred),
           ~ replace_na(., median(., na.rm = TRUE)))
  )

# drop factor columns with only ONE level
fac_cols  <- sapply(ames_clean, is.factor)
one_level <- names(ames_clean)[fac_cols][sapply(ames_clean[fac_cols], nlevels) < 2]

ames_mod <- ames_clean %>%
  select(-all_of(one_level),
         -Id)   # Id is just a label, not a predictor

```

Building design matrix:

```{r}
X <- model.matrix(
  log_price ~ . - SalePrice,  # all predictors except SalePrice (we use log_price)
  data = ames_mod
)[, -1]                        # drop intercept column; glmnet adds its own

y <- ames_mod$log_price
```


## Fitting the Model

```{r}
set.seed(123)

cv_lasso <- cv.glmnet(
  X, y,
  alpha  = 1,      # LASSO
  nfolds = 10
)

lambda_min <- cv_lasso$lambda.min
lambda_1se <- cv_lasso$lambda.1se

lambda_min
lambda_1se
```




Visualizing the CV curve: 

```{r}
plot(cv_lasso)
```


Fitting the final model: 

```{r}
lasso_model <- glmnet(X, y, alpha = 1, lambda = lambda_min)
```

Results: 

```{r}
coef_summary <- coef(lasso_model) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column("Variable") %>%
  rename(Coefficient = s0) %>%
  filter(Coefficient != 0) %>%
  arrange(desc(abs(Coefficient)))

coef_summary
```



## Testing

```{r}
# read test data
test <- read_csv("test_new.csv")

# handle missing values and character variables the same way as training
test_clean <- test %>%
  # replace na in character columns with "none"
  mutate(across(where(is.character), ~ replace_na(., "None"))) %>%
  # convert characters to factors
  mutate(across(where(is.character), factor))

```

```{r}
# compute training medians for numeric predictors
train_medians <- sapply(ames_clean[num_pred], median, na.rm = TRUE)

# apply median imputation to numeric predictors in test using training medians
for (v in num_pred) {
  if (v %in% names(test_clean)) {
    test_clean[[v]] <- replace_na(test_clean[[v]], train_medians[[v]])
  }
}

# drop the same single-level factor columns and id as in training
test_mod <- test_clean %>%
  select(-all_of(one_level),
         -Id)
```

```{r}
# build model matrix for test data (no response in formula)
X_test <- model.matrix(
  ~ . - SalePrice,   # same predictor structure as training
  data = test_mod
)[, -1]              # drop intercept column

# align test design matrix columns with training design matrix
train_cols <- colnames(X)
test_cols  <- colnames(X_test)

# add any columns that are in training but missing in test (fill with zeros)
missing_cols <- setdiff(train_cols, test_cols)
if (length(missing_cols) > 0) {
  X_test <- cbind(
    X_test,
    matrix(0, nrow(X_test), length(missing_cols),
           dimnames = list(NULL, missing_cols))
  )
}

# drop any extra columns that appear only in test
extra_cols <- setdiff(colnames(X_test), train_cols)
if (length(extra_cols) > 0) {
  X_test <- X_test[, setdiff(colnames(X_test), extra_cols)]
}

# reorder columns to match training matrix exactly
X_test <- X_test[, train_cols]

```

```{r}
# predict log-price on the test set
log_pred_test <- predict(lasso_model, newx = X_test, s = lambda_min)

# convert back from log scale to saleprice
saleprice_pred <- exp(log_pred_test)

```

```{r}
# extract true saleprice from test set
y_test <- test$SalePrice

# compute rmse (root mean squared error)
rmse <- sqrt(mean((y_test - saleprice_pred)^2))
rmse

# compute mae
mae <- mean(abs(y_test - saleprice_pred))
mae

# compute r-squared
sst <- sum((y_test - mean(y_test))^2)
sse <- sum((y_test - saleprice_pred)^2)
r_squared <- 1 - sse/sst
r_squared

```

```{r}
# create a results dataframe for visualization
results_df <- tibble(
  actual = test$SalePrice,
  predicted = as.numeric(saleprice_pred),
  residual = actual - predicted
)

# predicted vs actual plot
ggplot(results_df, aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(
    title = "Predicted vs Actual Sale Price (LASSO)",
    x = "Actual Sale Price",
    y = "Predicted Sale Price"
  )
```

## Conclusion

The LASSO model was trained on the training dataset and evaluated using a separate test dataset. Model performance on the test set was assessed using RMSE, MAE, and $R^2$. The model achieved an RMSE of \$31,110 and an MAE of \$15,727, indicating that predictions are typically within about \$16,000 of the true sale price, with some larger errors occurring. The test-set $R^2$ value of 0.85 shows that the model explains approximately 85% of the variability in housing prices on unseen data. Overall, these results indicate that the LASSO model generalizes well and provides strong predictive accuracy.


------------------------------------

# Model Analysis – Ridge Regression

## Motivation for Using Ridge

The EDA revealed:

- Many predictors are highly correlated (e.g., GarageCars with GarageArea, 1stFlrSF with TotalBsmtSF).

- The dataset contains many potential predictors (~79).

- Linear relationships appear strong, especially after transforming SalePrice.

- Multicollinearity is present among several structural features.


Unlike LASSO, Ridge regression:

- reduces coefficient variance through shrinkage

- stabilizes estimates in the presence of multicollinearity

- retains all predictors (no variable elimination)

Thus, Ridge regression is well-suited for this dataset when predictive accuracy is prioritized over model sparsity.


**The Ridge Model**

Given predictors $X_1, X_2, ..., X_p$ and response $y$, LASSO solves: 

$$
\hat{\beta} = \arg\min_\beta \left( \sum^n_{i=1}(y_i-\beta_0-\sum^p_{j=1} X_{ij} \beta_j)^2 + \lambda \sum^p_{j=1} \beta_j^2 \right)
$$

where $\lambda \ge 0$ controls shrinkage.


## Data Preparation 

Training data log-transform: 

```{r}
ames <- read_csv("train.csv") %>%
  mutate(
    log_price = log(SalePrice)
  )
```


Handling missing values: 

- for character predictors: replace NA with "None", then convert to factor
- for numeric predictors (other than Id, SalePrice, log_price): median impute

```{r}

ames_clean <- ames %>%
  # character -> "None" where NA
  mutate(across(where(is.character), ~ replace_na(., "None"))) %>%
  # now convert all characters to factors
  mutate(across(where(is.character), factor))

# numeric predictors (exclude Id, SalePrice, log_price)
num_pred <- names(ames_clean)[sapply(ames_clean, is.numeric) &
                                !(names(ames_clean) %in% c("Id", "SalePrice", "log_price"))]

ames_clean <- ames_clean %>%
  mutate(
    across(all_of(num_pred),
           ~ replace_na(., median(., na.rm = TRUE)))
  )

# drop factor columns with only ONE level
fac_cols  <- sapply(ames_clean, is.factor)
one_level <- names(ames_clean)[fac_cols][sapply(ames_clean[fac_cols], nlevels) < 2]

ames_mod <- ames_clean %>%
  select(-all_of(one_level),
         -Id)   # Id is just a label, not a predictor

```

Building design matrix:

```{r}
X <- model.matrix(
  log_price ~ . - SalePrice,  # all predictors except SalePrice (we use log_price)
  data = ames_mod
)[, -1]                        # drop intercept column; glmnet adds its own

y <- ames_mod$log_price
```


## Fitting the Ridge Model 

```{r}
set.seed(123)

cv_ridge <- cv.glmnet(
  X, y,
  alpha  = 0,      # ridge
  nfolds = 10
)

lambda_min_ridge <- cv_ridge$lambda.min
lambda_1se_ridge <- cv_ridge$lambda.1se

lambda_min_ridge
lambda_1se_ridge

```


Visualizing the CV curve: 

```{r}
plot(cv_ridge)
```


Fitting the final model: 

```{r}
ridge_model <- glmnet(
  X, y,
  alpha  = 0,
  lambda = lambda_min_ridge
)
```

Results: 

```{r}
coef_summary_ridge <- coef(ridge_model) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column("Variable") %>%
  rename(Coefficient = s0) %>%
  arrange(desc(abs(Coefficient)))

coef_summary_ridge

```

## Testing

```{r}
# read test data
test <- read_csv("test_new.csv")

# handle missing values and character variables the same way as training
test_clean <- test %>%
  # replace na in character columns with "none"
  mutate(across(where(is.character), ~ replace_na(., "None"))) %>%
  # convert characters to factors
  mutate(across(where(is.character), factor))

```

```{r}
# compute training medians for numeric predictors
train_medians <- sapply(ames_clean[num_pred], median, na.rm = TRUE)

# apply median imputation to numeric predictors in test using training medians
for (v in num_pred) {
  if (v %in% names(test_clean)) {
    test_clean[[v]] <- replace_na(test_clean[[v]], train_medians[[v]])
  }
}

# drop the same single-level factor columns and id as in training
test_mod <- test_clean %>%
  select(-all_of(one_level),
         -Id)
```

```{r}
# build model matrix for test data (no response in formula)
X_test <- model.matrix(
  ~ . - SalePrice,   # same predictor structure as training
  data = test_mod
)[, -1]              # drop intercept column

# align test design matrix columns with training design matrix
train_cols <- colnames(X)
test_cols  <- colnames(X_test)

# add any columns that are in training but missing in test (fill with zeros)
missing_cols <- setdiff(train_cols, test_cols)
if (length(missing_cols) > 0) {
  X_test <- cbind(
    X_test,
    matrix(0, nrow(X_test), length(missing_cols),
           dimnames = list(NULL, missing_cols))
  )
}

# drop any extra columns that appear only in test
extra_cols <- setdiff(colnames(X_test), train_cols)
if (length(extra_cols) > 0) {
  X_test <- X_test[, setdiff(colnames(X_test), extra_cols)]
}

# reorder columns to match training matrix exactly
X_test <- X_test[, train_cols]

```

```{r}
# predict log-price on the test set
log_pred_test_ridge <- predict(
  ridge_model,
  newx = X_test,
  s = lambda_min_ridge
)

# convert back from log scale to saleprice
saleprice_pred_ridge <- exp(log_pred_test_ridge)
```

```{r}
# compute rmse
rmse_ridge <- sqrt(mean((y_test - saleprice_pred_ridge)^2))
rmse_ridge

# compute mae
mae_ridge <- mean(abs(y_test - saleprice_pred_ridge))
mae_ridge

# compute r-squared
sst <- sum((y_test - mean(y_test))^2)
sse_ridge <- sum((y_test - saleprice_pred_ridge)^2)
r_squared_ridge <- 1 - sse_ridge/sst
r_squared_ridge
```

```{r}
# create a results dataframe for ridge visualization
results_df_ridge <- tibble(
  actual = test$SalePrice,
  predicted = as.numeric(saleprice_pred_ridge),
  residual = actual - predicted
)

# predicted vs actual plot
ggplot(results_df_ridge, aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(
    title = "Predicted vs Actual Sale Price (Ridge)",
    x = "Actual Sale Price",
    y = "Predicted Sale Price"
  )
```

## Conclusion

The Ridge model was trained on the training dataset and evaluated using a separate test dataset. Model performance on the test set was assessed using RMSE, MAE, and $R^2$. The model achieved an RMSE of \$42,352 and an MAE of \$28,826, indicating that predictions are typically within about \$29,000 of the true sale price, with some larger errors occurring. The test-set $R^2$ value of 0.72 shows that the model explains approximately 72% of the variability in housing prices on unseen data. Overall, these results indicate that the LASSO model generalizes well and provides strong predictive accuracy.




















# Model Analysis – PLS

## Motivation for Using Partial Least Squares

PLS could potentially handle the dataset well because it handles high correlation between variables (multicollinearity) well. We recognize that it also does a good job with data that has n > p (this is not the case with Ames data but still important to note). It also can shrink coefficients to zero, in turn performing variable selection. This is nice because with 79 predictors, there is a high chance of overfitting to the training set if not careful. 

Unlike the previous, Partial Least Squares:

- reduces coefficient variance even further through more shrinkage

- can remove (technically) predictors by shrinking the non-important predictors weights to zero

We know that this will *probably* not be the best when it comes to accuracy, but it can reaffirm us in what predictors are most important by looking at the largest and smallest weights given to each variable.


**The PLS Model**

Redundant yes but rebuilding to be sure on a fresh slate
```{r}
X <- model.matrix(
  log_price ~ . - SalePrice,  # all predictors except SalePrice (we use log_price)
  data = ames_mod
)[, -1]                        # drop intercept column; glmnet adds its own

y <- ames_mod$log_price
x_sd <- apply(X, 2, sd)
X_clean <- X[, x_sd > 0]

max_comp <- min(20, ncol(X_clean), nrow(X_clean) - 1)
```

### Fitting initial model and visualizing outputs
``` {r}
fit.pls <- mvr(
  y ~ X,
  ncomp = max_comp,          
  scale = TRUE,
  validation = "none",
  segments = 10,
  segment.type = "consecutive"
)

summary(fit.pls)
validationplot(fit.pls, val.type="RMSEP")
```
It looks like 5-6 PLS components are suitable for fitting the Ames dataset. The Root Mean Squared Error of Prediction (sqrt(MSEP)) turns into overfitting after that and since house prices can be highly variable. 6 components is not that much and then we can explain > 92% of sale price, so we will select 6 components.

### Tuning
```{r}
# 6 components
final.pls <- mvr(
  y ~ X,
  ncomp = max_comp,          
  scale = TRUE,
  validation = "none",
  segments = 10,
  segment.type = "consecutive"
)

# Predict on TEST data
pls.pred <- predict(final.pls, newdata = X_test, ncomp = 6)

# Convert back to normal
pls.pred <- exp(pls.pred)

# Calculate test error
test_mse <- mean((as.vector(pls.pred) - y_test)^2)
test_rmse <- sqrt(test_mse)

print(paste("Test RMSE:", round(test_rmse, 2)))
print(paste("Test MSE:", round(test_mse, 2)))
print(paste("Error as %: ", round(test_rmse*100/180921, 2)))


# compute r-squared
sst <- sum((y_test - mean(y_test))^2)
sse_pls <- sum((y_test - saleprice_pred_ridge)^2)
r_squared_pls <- round(1 - sse_pls/sst, 2)
print(paste("R^2: ", r_squared_pls))
```
```{r}
# Get loading weights
loadings <- loading.weights(final.pls)

# For the first component (usually most important)
comp1_loadings <- loadings[, 1]
comp1_sorted <- sort(abs(comp1_loadings), decreasing = TRUE)
print(head(comp1_sorted, 15))

# Visualize
barplot(head(comp1_sorted, 15), las=2, 
        main="Top Variables - Component 1 Loading Weights",
        cex.names=0.7, col="darkgreen")
```



## Conclusion
As expected, it was not the best in accuracy metrics. But nonetheless it is helpful and reaffirms our beliefs of the most important predictors (OverallQual, GrLivArea, GarageCars, etc.). The testing RMSE was 42829 and had an R^2 of 0.72. These values were pretty close to the Ridge Regression model, which makes sense because they both excel in the same kind of qualities. Because it is so similar to the previous model, a different visualization was chosen to capture the weights of coefficients instead of the predictions. Finally, using the mean SalePrice and our testing RMSE, we can calculate the average percent we were off by in predictions: 23.7%. Overall, these results indicate that the PLS model indeed fit the data well, but not the best possible (in comparison).








